{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "import nltk\n",
    "import plotly\n",
    "import gensim\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data_5_mins_pos_pol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotly.tools.set_credentials_file(username='gasia4444', api_key='rC8FB3pveGzyME366dL7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.set_value(index, 'timestamp', pd.Timestamp(row['timestamp'], unit='s').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topic = {}\n",
    "for tit in tqdm(grouped_topics.keys()):\n",
    "    sentence = (df.message_text[grouped_topics[tit]].values)\n",
    "    dict_topic.update({tit: ''.join(str(v) for v in sentence) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_message = []\n",
    "for i in dict_topic.keys():\n",
    "    list_message.append(dict_topic[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  #match any word characters until it reaches a non-word character, like a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from tokens\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "# print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['will','s' , 'nbsp', 't', 'com', 'http', 'amp', '1xnk8bc', 'href', 'oto', 'www' ]\n",
    "stopped_tokens = [i for i in stopped_tokens if not i in remove_words]\n",
    "# print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# Create lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "remove_words = ['will','s' ,'bitcoin', 'just', 'get', 'use', 'now', 'solidx' 'people','bitfinex', 'think', 'maybe', 'imageshack', 'pt', 'em', 'img', 'nbsp', 't', 'com', 'http', 'amp', '1xnk8bc', 'href', 'oto', 'www', 'isn', 'etc', 'etf', 'tr', 'td', 'img', 'ath', 'xt', 'xp', 'php', 'img', 'gt', 'pboc', 'th', 'mtgox', 'cny', 'huobi',\n",
    " 'm', 'import', 'st', 'lt', 'zhou' 'ok', 'color', 'can', \"adam\", \"bitcoin\", 'import', 'http', 'li', 'b', 'style' , 'font', 're', 'le', 'gif','span','hr', 'd' , 'jpg', 'png',  'am5om', 'fud', 'mt', 'th' 'hfebupaeo', 'ftdata', 'zbb', 'imgur', 'bite', 'uztgwi', 'podomatic']\n",
    "\n",
    "map_words = {\n",
    "    'btc': 'bitcoin',\n",
    "    'bcc': 'bitcoin',\n",
    "    'gbtc': 'bitcoin',\n",
    "    'bitcoinca': 'bitcoin',\n",
    "    'better': 'good',\n",
    "    'increase': 'rise',\n",
    "    'miner': 'mine',\n",
    "    'winner': 'win'\n",
    "}\n",
    "\n",
    "bow_dict = {}\n",
    "for m, raw in df.iterrows():\n",
    "    r = str(raw['messages']).lower()\n",
    "    tokens = tokenizer.tokenize(r)\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stopped_tokens = [i for i in stopped_tokens if not i in remove_words]\n",
    "\n",
    "    #   lemmatize tokents:  \n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'a') for i in stopped_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'n') for i in lemmatized_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'v') for i in lemmatized_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 'r') for i in lemmatized_tokens]\n",
    "    lemmatized_tokens = [wnl.lemmatize(i, 's') for i in lemmatized_tokens]\n",
    "    \n",
    "    #     stemmed_tokens = [p_stemmer.stem(i) for i in lemmatized_tokens]\n",
    "    \n",
    "    #   remove words:\n",
    "    stemmed_tokens = [i for i in lemmatized_tokens if not i in remove_words]\n",
    "    stemmed_tokens = [i for i in stemmed_tokens if not i in en_stop]\n",
    "    d = pd.DataFrame({'z': stemmed_tokens})\n",
    "    stemmed_tokens = d.replace(map_words)['z'].tolist()\n",
    "    \n",
    "    #   remove numeric values:\n",
    "    alpha_only = [''.join(filter(str.isalpha, i))  for i in stemmed_tokens if len(''.join(filter(str.isalpha, i))) >1]\n",
    "    bow_dict.update({m:alpha_only})\n",
    "    texts.append(alpha_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import datetime\n",
    "\n",
    "a = datetime.datetime.now()\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep bag of words in order to get docements for each of the topic\n",
    "for i, r in df.iterrows():\n",
    "    bow = dictionary.doc2bow(bow_dict[i])\n",
    "    corpus.append(bow)\n",
    "    bow_dict.update({i:bow})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = datetime.datetime.now()\n",
    "# damodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=20)\n",
    "# b = datetime.datetime.now()\n",
    "# c = b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(damodel, 'LDA_model_last.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "damodel = joblib.load('LDA_model_last.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = damodel.get_document_topics(bow_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = [dict(damodel.get_document_topics(bow_dict[i])) for i in range((len(bow_dict)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa_df = pd.DataFrame(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa_df = aa_df.fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['lda_0'] = aa_df[0]\n",
    "df['lda_1'] = aa_df[1]\n",
    "df['lda_2'] = aa_df[2]\n",
    "df['lda_3'] = aa_df[3]\n",
    "df['lda_4'] = aa_df[4]\n",
    "df['lda_5'] = aa_df[5]\n",
    "df['lda_6'] = aa_df[6]\n",
    "df['lda_7'] = aa_df[7]\n",
    "df['lda_8'] = aa_df[8]\n",
    "df['lda_9'] = aa_df[9]\n",
    "df['lda_10'] = aa_df[10]\n",
    "df['lda_11'] = aa_df[11]\n",
    "df['lda_12'] = aa_df[12]\n",
    "df['lda_13'] = aa_df[13]\n",
    "df['lda_14'] = aa_df[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('all_data_lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damodel.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get LDA topics:\n",
    "for i in damodel.print_topics(num_topics=50, num_words=10):\n",
    "    print(i)\n",
    "    print('-----------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
